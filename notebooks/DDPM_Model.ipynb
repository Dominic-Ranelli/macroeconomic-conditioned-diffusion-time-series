{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DDPM setup...\n",
      "using device: cpu\n",
      "Configuration loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conditional DDPM for Financial Wavelet Inputs\n",
    "\n",
    "Model each asset window as a 2D time-frequency representation (wavelet scalogram) and \n",
    "train a conditional diffusion model to generate realistic return dynamics in the\n",
    "wavelet domain, conditioned on macroeconomic information.\n",
    "\n",
    "Primary artifacts:\n",
    "- X_all: (T, N_assets, N_scales) wavelet coefficients\n",
    "- C_all: (T, C_features) conditioning variables\n",
    "\n",
    "Structured as a production ML experiment:\n",
    "- explicit configuration\n",
    "- dataset with leakage-aware splits\n",
    "- U-Net backbone with time + conditioning embeddings\n",
    "- diffusion wrapper + training loop + EMA\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Use GPU if possible, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('using device:', device)\n",
    "\n",
    "# We then set base hyperparameters within a callable configuration\n",
    "CONFIG = {\n",
    "    'paths': {\n",
    "        'export_dir': 'artifacts_all',\n",
    "        'checkpoint_dir': '/home/dsranelli/bigproject/checkpoint_dir'\n",
    "    },\n",
    "    'data': {\n",
    "        'window': 128,\n",
    "        'width': 128,\n",
    "        'train_split': 0.8,\n",
    "        'stride': 1,\n",
    "        'asset_limit': 10,\n",
    "        'filter_energy_sigma': 5.0,  # <-- ADD THIS LINE\n",
    "    },\n",
    "    'model': {\n",
    "        \"input_channels\": 1,\n",
    "        \"base_channels\": 128,\n",
    "        'time_emb_dim': 128,\n",
    "    },\n",
    "    'diffusion': {\n",
    "        'timesteps': 1000,\n",
    "        'beta_start': 1e-4,\n",
    "        'beta_end': 0.02,\n",
    "        'lambda_x0': 0.1,\n",
    "        'lambda_spec': 0.01\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 128,\n",
    "        'num_epochs': 300,\n",
    "        'learning_rate': 3e-5,\n",
    "        'weight_decay': 0.0,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'early_stop_patience': 20,\n",
    "        'dropout_rate': 0.1\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    \"\"\"\n",
    "    X_all: np.ndarray\n",
    "        Wavelet tensor of shape (T, Number of assets, Number of scales)\n",
    "    C_all: np.ndarray\n",
    "        Conditioning matrix of shape (T, conditional features)\n",
    "    date_index: np.ndarray\n",
    "        Array of date strings with shape (T,)\n",
    "    N_ASSETS: int\n",
    "        Number of assets included\n",
    "    C_SCALES: int\n",
    "        Conditioning feature dimension\n",
    "    N_SCALES: int\n",
    "        Wavelet scale dimension (height of scalogram)  \n",
    "    \"\"\"\n",
    "    paths = config[\"paths\"]\n",
    "    export_dir = paths[\"export_dir\"]\n",
    "    asset_limit = config[\"data\"].get(\"asset_limit\", None)\n",
    "\n",
    "    # Creating asset universe with metadata\n",
    "    with open(os.path.join(export_dir, \"basket_assets.json\"), \"r\") as f:\n",
    "        basket_assets = json.load(f)\n",
    "    N_ASSETS_FULL = len(basket_assets)\n",
    "\n",
    "    # We load our asset universe\n",
    "    X_all = np.load(os.path.join(export_dir, \"X_all.npy\"), allow_pickle=True)\n",
    "\n",
    "    \n",
    "    # Squeeze into correct data shape\n",
    "    X_raw = X_all.astype(np.float32)\n",
    "    print(\"Loaded wavelet array:\", X_raw.shape, \"ndim:\", X_raw.ndim, \"size:\", X_raw.size)\n",
    "\n",
    "\n",
    "    mask = np.isfinite(X_raw)\n",
    "    X_mean = X_raw[mask].mean()\n",
    "    X_std  = X_raw[mask].std() + 1e-8\n",
    "    \n",
    "    \n",
    "    X_raw = np.where(mask, X_raw, X_mean).astype(np.float32)\n",
    "\n",
    "    X_all = (X_raw - X_mean) / X_std\n",
    "    X_all = np.clip(X_all, -10, 10)\n",
    "\n",
    "\n",
    "    # We then normalize all entire asset values\n",
    "    X_mean = X_all.mean()\n",
    "    X_std  = X_all.std() + 1e-8\n",
    "    print(f\"Global X mean/std before norm: {X_mean:.4f}, {X_std:.4f}\")\n",
    "    X_all = (X_all - X_mean) / X_std\n",
    "\n",
    "    config[\"data\"][\"x_mean\"] = float(X_mean)\n",
    "    config[\"data\"][\"x_std\"]  = float(X_std)\n",
    "\n",
    "    # We then flatten if needed\n",
    "    if X_all.ndim == 3:\n",
    "        X_all = X_all.reshape(X_all.shape[0], -1)\n",
    "\n",
    "    T, TOTAL_X_FEAT = X_all.shape\n",
    "    EXPECTED_HEIGHT = 32\n",
    "    EXPECTED_TOTAL = N_ASSETS_FULL * EXPECTED_HEIGHT\n",
    "\n",
    "    # Reshape our values once more\n",
    "    X_all = X_all.reshape(T, N_ASSETS_FULL, N_SCALES)\n",
    "    \n",
    "    # The we slice our assets\n",
    "    if asset_limit and asset_limit < N_ASSETS_FULL:\n",
    "        print(f\"Slicing X_all from {N_ASSETS_FULL} to {asset_limit} assets.\")\n",
    "        X_all = X_all[:, :asset_limit, :]\n",
    "        N_ASSETS = asset_limit\n",
    "    else:\n",
    "        N_ASSETS = N_ASSETS_FULL\n",
    "    X_all = np.nan_to_num(X_all, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    X_all = np.clip(X_all, -10, 10)\n",
    "\n",
    "    # Then we load our conditioning matrix\n",
    "    C_all = np.load(os.path.join(export_dir, \"C_all.npy\"), allow_pickle=True)\n",
    "\n",
    "    if C_all.ndim == 3:\n",
    "        C_all = C_all.reshape(C_all.shape[0], -1)\n",
    "\n",
    "    T_C, TOTAL_C_FEAT = C_all.shape\n",
    "\n",
    "    \n",
    "    if TOTAL_C_FEAT % N_ASSETS_FULL == 0:\n",
    "        C_SCALES = TOTAL_C_FEAT // N_ASSETS_FULL\n",
    "        C_all = C_all.reshape(T_C, N_ASSETS_FULL, C_SCALES)\n",
    "        if asset_limit and asset_limit < N_ASSETS_FULL:\n",
    "            C_all = C_all[:, :asset_limit, :]\n",
    "        print(f\"C_all shape (asset-specific): {C_all.shape}\")\n",
    "    else:\n",
    "        C_SCALES = TOTAL_C_FEAT\n",
    "        print(f\"C_all shape (market-wide): {C_all.shape}\")\n",
    "\n",
    "    C_all = np.nan_to_num(C_all, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    C_all = np.clip(C_all, -10, 10)\n",
    "\n",
    "    # We align all our information within the dates\n",
    "    date_index = np.load(os.path.join(export_dir, \"date_index.npy\"), allow_pickle=True)\n",
    "    date_index = np.array(date_index, dtype=str)\n",
    "\n",
    "    print(f\"X_all final: {X_all.shape}, C_all final: {C_all.shape}\")\n",
    "    RAW_DIR = \"/home/dsranelli/bigproject/artifacts_all/raw\"\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "    return X_all, C_all, date_index, N_ASSETS, C_SCALES, N_SCALES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletDDPMDataset(Dataset):\n",
    "    def __init__(self, config, X_all, C_all, N_ASSETS, C_SCALES):\n",
    "        \"\"\"\n",
    "        Returns training samples in form of both:\n",
    "        x: assets (1, H, W)\n",
    "        c: (C_features)\n",
    "\n",
    "        We treat wavelet slice as an image so the U-Net can learn multi-scale structure.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.X_all = X_all\n",
    "        self.C_all = C_all\n",
    "        self.W = config['data']['window']\n",
    "        self.stride = config['data']['stride']\n",
    "        self.N_ASSETS = N_ASSETS\n",
    "        self.N_SCALES = X_all.shape[2]\n",
    "        \n",
    "        self.T = self.X_all.shape[0]\n",
    "        self.time_starts = list(range(0, self.T - self.W + 1, self.stride))\n",
    "        self.num_time_windows = len(self.time_starts)\n",
    "\n",
    "        self.samples = []\n",
    "        epsilon_1 = 1e-6  # tune\n",
    "        for time_idx in range(self.num_time_windows):\n",
    "            t_start = self.time_starts[time_idx]\n",
    "            t_end   = t_start + self.W\n",
    "            for asset_idx in range(N_ASSETS):\n",
    "\n",
    "                x_window = self.X_all[t_start:t_end, asset_idx, :].T\n",
    "                energy = np.sqrt((x_window**2).mean())\n",
    "                if energy > epsilon_1:\n",
    "                    self.samples.append((time_idx, asset_idx))\n",
    "                    \n",
    "        print(f\"Dataset: {self.num_time_windows} windows × {self.N_ASSETS} assets = {len(self.samples)} samples (after filtering)\")\n",
    "        print(f\"C_all shape: {C_all.shape}, C_all ndim: {C_all.ndim}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get time window and asset index\n",
    "        time_idx, asset_idx = self.samples[idx]\n",
    "        t_start = self.time_starts[time_idx]\n",
    "        t_end = t_start + self.W\n",
    "        \n",
    "        # Find window for wavelet\n",
    "        x_window = self.X_all[t_start:t_end, asset_idx, :].T \n",
    "\n",
    "        # Find energy to filter out poor visuals\n",
    "        energy = np.sqrt((x_window **2).mean())\n",
    "\n",
    "        # Create conditioning window, reduced to one vector per sample\n",
    "        if self.C_all.ndim == 2:\n",
    "            c_window = self.C_all[t_start:t_end, :]\n",
    "            c_reduced = c_window.mean(axis=0)\n",
    "        else:\n",
    "            c_window = self.C_all[t_start:t_end, asset_idx, :]\n",
    "            c_reduced = c_window.mean(axis=0)\n",
    "        \n",
    "        c_full = np.concatenate(\n",
    "            [c_reduced.astype(np.float32), np.array([energy], dtype=np.float32)],\n",
    "            axis=0\n",
    "        ) \n",
    "        x_tensor = torch.from_numpy(x_window).float().unsqueeze(0)\n",
    "        c_tensor = torch.from_numpy(c_full).float()\n",
    "\n",
    "        return x_tensor, c_tensor\n",
    "\n",
    "def build_dataloaders(config, X_all, C_all, N_ASSETS, C_SCALES):\n",
    "    dataset = WaveletDDPMDataset(config, X_all, C_all, N_ASSETS, C_SCALES)\n",
    "    train_split = config['data']['train_split']\n",
    "\n",
    "    # Split by TIME to prevent leakage\n",
    "    total_windows = dataset.num_time_windows\n",
    "    train_split = float(config[\"data\"][\"train_split\"])\n",
    "\n",
    "    split_idx = int(train_split * total_windows)\n",
    "    split_idx = max(1, min(split_idx, total_windows - 1))\n",
    "\n",
    "    train_indices = [i for i, (t, a) in enumerate(dataset.samples) if t < split_idx]\n",
    "    val_indices   = [i for i, (t, a) in enumerate(dataset.samples) if t >= split_idx]\n",
    "\n",
    "\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    val_subset   = Subset(dataset, val_indices)\n",
    "\n",
    "    batch_size = config[\"training\"][\"batch_size\"]\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True,  drop_last=True)\n",
    "    val_loader   = DataLoader(val_subset,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Embedding for Diffusion Timesteps\n",
    "\n",
    "    Diffusion models condition the denoising network on the current timestep `t`.\n",
    "    Maps integer timesteps into a continuous vector using sine/cosine features (similar to transformer embeddings).\n",
    "\n",
    "    Input\n",
    "    t : torch.Tensor\n",
    "        Shape (B,) or (B, 1). Typically integer timesteps from [0, T-1].\n",
    "\n",
    "    Output\n",
    "    emb : torch.Tensor\n",
    "        Shape (B, dim). No learned parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    # Computes sinusoidal embedding for timesteps\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        freqs = torch.exp(\n",
    "            torch.linspace(\n",
    "                math.log(1.0),\n",
    "                math.log(10000.0),\n",
    "                half_dim,\n",
    "                device = device))\n",
    "        # Conver to float for division\n",
    "        args = t.float().unsqueeze(1) / freqs.unsqueeze(0)\n",
    "        # Concatenates sin and cos featues\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim =-1)\n",
    "        # If odd, pad to reach dimension\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = torch.cat([emb, emb[:, :1]], dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Residual Block\n",
    "class ResBlock2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Block with Time/Conditioning Embedding\n",
    "\n",
    "    Fundamental unit of the conditional U-Net within the diffusion model. \n",
    "    It processes a 2D feature map (wavelet scalogram)\n",
    "    while incorporating a per-sample conditioning embedding (time step + macro/fundamental).\n",
    "\n",
    "    Design choices:\n",
    "    - Residual connections stabilize training in deep diffusion models.\n",
    "    - Conditioning injected additively after the first convolution,\n",
    "      standard in DDPM-style architectures and empirically stable.\n",
    "    - GroupNorm is preferred over BatchNorm as diffusion models are trained\n",
    "      with small or variable batch sizes.\n",
    "    - SiLU is used in diffusion and transformer models\n",
    "      due to smoother gradients than ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, emb_dim, kernel_size=3, dropout = 0.1):\n",
    "         \"\"\"\n",
    "        in_ch:\n",
    "            # of input feature channels.\n",
    "        out_ch:\n",
    "            # of output feature channels.\n",
    "        emb_dim:\n",
    "            Dimensionality of the conditioning embedding (time + macro features).\n",
    "        kernel_size:\n",
    "            Spatial kernel size for convolutions.\n",
    "        dropout:\n",
    "            Dropout probability applied after the second convolution.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Main conv path\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size, padding=padding)\n",
    "        self.gn1   = nn.GroupNorm(8, out_ch)\n",
    "        self.act   = nn.SiLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size, padding=padding)\n",
    "        self.gn2   = nn.GroupNorm(8, out_ch)\n",
    "\n",
    "        # Conditioning projection\n",
    "        # Projects conditioning embedding (timestep and macro features) into channel space to be broadcasted and added to feature map \n",
    "        self.emb_proj = nn.Linear(emb_dim, out_ch)\n",
    "\n",
    "        # Skip connection\n",
    "        # If number of channels changes, project input to match output dimensions\n",
    "        if in_ch != out_ch:\n",
    "            self.skip = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout2d(dropout)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Parameters:\n",
    "        x:\n",
    "            Input feature map of shape (B, in_ch, H, W).\n",
    "            H = number of wavelet scales and W = time window length.\n",
    "        emb:\n",
    "            Conditioning embedding of shape (B, emb_dim).\n",
    "            A learned combination of:\n",
    "              - diffusion time-step embedding\n",
    "              - macro/fundamental/regime conditioning features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output:\n",
    "            Output feature map of shape (B, out_ch, H, W).\n",
    "        \"\"\"\n",
    "\n",
    "        # First conv block\n",
    "        h = self.conv1(x)\n",
    "        h = self.gn1(h)\n",
    "        h = self.act(h)\n",
    "\n",
    "        # Project embedding and broadcasted across spatial dimensions\n",
    "        emb_out = self.emb_proj(emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        h = h + emb_out  # (B, out_ch, H, W)\n",
    "\n",
    "        # Second conv block\n",
    "        h = self.conv2(h)\n",
    "        h = self.gn2(h)\n",
    "        h = self.act(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        # Skip connection\n",
    "        skip = self.skip(x)\n",
    "        return h + skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET2DCond(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional 2D U-Net for DDPM Noise Prediction\n",
    "\n",
    "    Predicts noise component epsilon added at diffusion timestep t, given:\n",
    "      - x_t: a noisy 2D image representation (wavelet scalogram)\n",
    "      - t: diffusion timestep\n",
    "      - cond: conditioning vector (macro information)\n",
    "\n",
    "    Input/Output contract (typical for this project)\n",
    "    x : (B, 1, H, W)  where H = N_scales (e.g., 32) & W = window length (e.g., 128)\n",
    "    t : (B,) integer timesteps in [0, T-1]\n",
    "    cond : (B, cond_dim) conditioning vector\n",
    "\n",
    "    returns:\n",
    "    eps_hat : (B, 1, H, W) predicted noise\n",
    "\n",
    "    Notes (signals ML diffusion competence)\n",
    "    - U-Net with skip connections to preserve fine-scale detail\n",
    "    - ResBlocks inject conditioning embeddings at multiple resolutions\n",
    "    - GroupNorm + SiLU are standard diffusion choices\n",
    "    \"\"\"   \n",
    "    \n",
    "    def __init__(self, in_channels, cond_dim, base_channels=64, time_emb_dim=128, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding\n",
    "        # Sinusoidal embeddint and small multi-layer perception \n",
    "        # Provides smooth representtion of diffusion noise levels at timestep t\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim * 2, time_emb_dim)\n",
    "        )\n",
    "\n",
    "        # Conditional embedding\n",
    "        # Maps macro feartures into same embedding space to combine with timestep embedding\n",
    "        self.cond_mlp = nn.Sequential(\n",
    "            nn.Linear(cond_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "\n",
    "        emb_dim = time_emb_dim\n",
    "\n",
    "        # Input projection\n",
    "        self.in_conv = nn.Conv2d(in_channels, base_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Downsampling path - FIXED\n",
    "        # Use pairs of ResBlocks at each resolution, then strided conv for downsampling\n",
    "        self.down1 = ResBlock2d(base_channels, base_channels, emb_dim)\n",
    "        self.down2 = ResBlock2d(base_channels, base_channels, emb_dim)\n",
    "        self.downsample1 = nn.Conv2d(base_channels, base_channels * 2, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.down3 = ResBlock2d(base_channels * 2, base_channels * 2, emb_dim)\n",
    "        self.down4 = ResBlock2d(base_channels * 2, base_channels * 2, emb_dim)\n",
    "        self.downsample2 = nn.Conv2d(base_channels * 2, base_channels * 4, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.down5 = ResBlock2d(base_channels * 4, base_channels * 4, emb_dim)\n",
    "        self.down6 = ResBlock2d(base_channels * 4, base_channels * 4, emb_dim)\n",
    "        self.downsample3 = nn.Conv2d(base_channels * 4, base_channels * 8, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Bottleneck (lowest resolution)\n",
    "        self.bot1 = ResBlock2d(base_channels * 8, base_channels * 8, emb_dim)\n",
    "        self.bot2 = ResBlock2d(base_channels * 8, base_channels * 8, emb_dim)\n",
    "\n",
    "        # Upsampling path\n",
    "        # Upsample, concat skip activations, and use ResBlock to fuse\n",
    "        self.upsample1 = nn.ConvTranspose2d(base_channels * 8, base_channels * 4, kernel_size=4, stride=2, padding=1)\n",
    "        self.up1 = ResBlock2d(base_channels * 8, base_channels * 4, emb_dim)\n",
    "        \n",
    "        self.upsample2 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, kernel_size=4, stride=2, padding=1)\n",
    "        self.up2 = ResBlock2d(base_channels * 4, base_channels * 2, emb_dim)\n",
    "        \n",
    "        self.upsample3 = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.up3 = ResBlock2d(base_channels * 2, base_channels, emb_dim)\n",
    "\n",
    "        # Final projection (back to noise prediction channel)\n",
    "        self.out_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)  # Output: (B, 1, H, W)\n",
    "        )\n",
    "        if dropout_rate > 0:\n",
    "            self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        else:\n",
    "            self.dropout_rate = nn.Identity()\n",
    "    \n",
    "\n",
    "    def forward(self, x, t, cond):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x:\n",
    "            Noisy input scalogram, shape of (B, in_channels, H, W)\n",
    "        t:\n",
    "            Diffusion step indices, shape of (B,) or (B,1)\n",
    "        cond:\n",
    "            Conditioning features, shape of (B, cond_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        eps_hat:\n",
    "            Predicted noise tensor, shape (B, in_channels, H, W)\n",
    "        \"\"\"\n",
    "\n",
    "        # Building combined embedding used by all ResBlocks\n",
    "        t_emb = self.time_mlp(t)\n",
    "        c_emb = self.cond_mlp(cond)\n",
    "        emb = t_emb + c_emb\n",
    "\n",
    "        # Encoder\n",
    "        x0 = self.in_conv(x)\n",
    "        \n",
    "        # Down path with skip connections\n",
    "        d1 = self.down1(x0, emb)  # (B, 64, 32, 128)\n",
    "        d2 = self.down2(d1, emb)  # (B, 64, 32, 128)\n",
    "        d3 = self.downsample1(d2)  # (B, 128, 16, 64)\n",
    "        \n",
    "        d4 = self.down3(d3, emb)  # (B, 128, 16, 64)\n",
    "        d5 = self.down4(d4, emb)  # (B, 128, 16, 64)\n",
    "        d6 = self.downsample2(d5)  # (B, 256, 8, 32)\n",
    "        \n",
    "        d7 = self.down5(d6, emb)  # (B, 256, 8, 32)\n",
    "        d8 = self.down6(d7, emb)  # (B, 256, 8, 32)\n",
    "        x = self.downsample3(d8)  # (B, 512, 4, 16)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bot1(x, emb)  # (B, 512, 4, 16)\n",
    "        x = self.bot2(x, emb)  # (B, 512, 4, 16)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        x = self.upsample1(x)  # (B, 256, 8, 32)\n",
    "        x = torch.cat([x, d8], dim=1)  # (B, 512, 8, 32)\n",
    "        x = self.up1(x, emb)  # (B, 256, 8, 32)\n",
    "        \n",
    "        x = self.upsample2(x)  # (B, 128, 16, 64)\n",
    "        x = torch.cat([x, d5], dim=1)  # (B, 256, 16, 64)\n",
    "        x = self.up2(x, emb)  # (B, 128, 16, 64)\n",
    "        \n",
    "        x = self.upsample3(x)  # (B, 64, 32, 128)\n",
    "        x = torch.cat([x, d2], dim=1)  # (B, 128, 32, 128)\n",
    "        x = self.up3(x, emb)  # (B, 64, 32, 128)\n",
    "\n",
    "        # Final projection\n",
    "        return self.out_conv(x)  # (B, 1, 32, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion Model\n",
    "class GaussianDiffusion2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian Diffusion Wrapper (DDPM) for 2D Inputs (B, C, H, W)\n",
    "\n",
    "    Implements:\n",
    "    - Forward process q(x_t | x_0): add noise according to beta schedule\n",
    "    - Training objective: predict noise epsilon (using epsilon-prediction)\n",
    "    - Reverse process p(x_{t-1} | x_t): iterative denoising sampler\n",
    "\n",
    "    Notes\n",
    "    - Classic DDPM setup with a linear beta schedule.\n",
    "    - Include optional auxiliary losses:\n",
    "        - x0 reconstruction (lambda_x0)\n",
    "        - spectral consistency along the time axis (lambda_spec)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        timesteps: int = 100,\n",
    "        beta_start: float = 1e-4,\n",
    "        beta_end: float = 0.02,\n",
    "        device: str = \"cpu\",\n",
    "        lambda_x0: float = 0.0,\n",
    "        lambda_spec: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.timesteps = timesteps\n",
    "        self.lambda_x0 = lambda_x0\n",
    "        self.lambda_spec = lambda_spec\n",
    "\n",
    "        # Noise schedule (linear beta)\n",
    "        betas = torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float32)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "        # Buffers with modeule\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas\", alphas)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer(\n",
    "            \"sqrt_one_minus_alphas_cumprod\",\n",
    "            torch.sqrt(1.0 - alphas_cumprod),\n",
    "        )\n",
    "\n",
    "    def q_sample(\n",
    "        self,\n",
    "        x_start: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        noise: torch.Tensor | None = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_t from the forward process q(x_t | x_0, t):\n",
    "\n",
    "            x_t = sqrt(a-bar_t) * x_0 + sqrt(1 - a-bar_t) * epsilon,   epsilon ~ N(0, I)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_start:\n",
    "            Clean input x_0 of shape (B, C, H, W).\n",
    "        t:\n",
    "            Timesteps (B,) is of dtype long.\n",
    "        noise:\n",
    "            Optional noise tensor epsilon. If None, sampled from standard Normal.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_t:\n",
    "            Noisy input at timestep t, shape (B, C, H, W).\n",
    "        noise:\n",
    "            The noise epsilon used.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        # Per-sample scalars and reshape for broadcast\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "\n",
    "        x_t = sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        return x_t, noise\n",
    "    \n",
    "    def predict_start_from_noise(self, x_t: torch.Tensor, t: torch.Tensor, noise: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reconstruct x0 from x_t and predicted noise epsilon:\n",
    "\n",
    "            x0 = (x_t - sqrt(1 - a-bar_t) * epsilon) / sqrt(a-bar_t)\n",
    "        \"\"\" \n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "    \n",
    "        x0_pred = (x_t - sqrt_one_minus_alphas_cumprod_t * noise) / (sqrt_alphas_cumprod_t + 1e-8)\n",
    "        return x0_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def _spectral_loss(\n",
    "        x_fake: torch.Tensor,\n",
    "        x_real: torch.Tensor,\n",
    "        eps: float = 1e-8\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Spectral loss along the time axis (dimension W).\n",
    "\n",
    "        For wavelet scalograms, we care that generated samples preserve\n",
    "        temporal frequency content. We match log-magnitude spectra using rFFT.\n",
    "\n",
    "        x_*: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        # Loss must be applied to the time dimension (W)\n",
    "        Xf = torch.fft.rfft(x_fake, dim=-1)\n",
    "        Xr = torch.fft.rfft(x_real, dim=-1)\n",
    "        mag_f = torch.log(torch.abs(Xf) + eps)\n",
    "        mag_r = torch.log(torch.abs(Xr) + eps)\n",
    "        return F.mse_loss(mag_f, mag_r)\n",
    "\n",
    "        # Training loss (epsilon prediction and auxiliary terms\n",
    "    def p_losses(self, x_start, cond, t=None):\n",
    "        \"\"\"\n",
    "        Computes DDPM training loss.\n",
    "\n",
    "        Primary objective (epsilon prediction):\n",
    "            L_eps = ||epsilon_theta(x_t, t, cond) - epsilon||^2\n",
    "\n",
    "        \"\"\"\n",
    "        if t is None:\n",
    "            t = torch.randint(0, self.timesteps, (x_start.size(0),), device=x_start.device)\n",
    "\n",
    "        noise = torch.randn_like(x_start)\n",
    "        x_noisy, _ = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        eps_pred = self.model(x_noisy, t, cond)\n",
    "\n",
    "        # Predict the noise with the conditional U-Net\n",
    "        loss_eps = F.mse_loss(eps_pred, noise)\n",
    "\n",
    "        # Weighted epsilon loss \n",
    "        weights = 1.0 / (1.0 - self.alphas_cumprod[t] + 1e-8)\n",
    "        weights = weights.view(-1, 1, 1, 1)\n",
    "\n",
    "        loss_eps = F.mse_loss(eps_pred, noise, reduction='none')\n",
    "        loss_eps = (weights * loss_eps).mean()\n",
    "\n",
    "\n",
    "        # x_0 reconstruction\n",
    "        loss_x0 = 0.0\n",
    "        if self.lambda_x0 > 0:\n",
    "            x0_pred = self.predict_start_from_noise(x_noisy, t, eps_pred)\n",
    "            loss_x0 = F.mse_loss(x0_pred, x_start)\n",
    "\n",
    "        # Spectral term along time axis\n",
    "        loss_spec = 0.0\n",
    "        if self.lambda_spec > 0:\n",
    "            real_spec = torch.fft.rfft(x_start, dim=-1).abs()\n",
    "            gen_spec = torch.fft.rfft(x0_pred, dim=-1).abs()\n",
    "            loss_spec = F.mse_loss(gen_spec, real_spec)\n",
    "\n",
    "        return loss_eps + self.lambda_x0 * loss_x0 + self.lambda_spec * loss_spec\n",
    "\n",
    "\n",
    "    # ---------- reverse diffusion for sampling ----------\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x_t: torch.Tensor, t: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reverse step.\n",
    "\n",
    "        For DDPM with epsilon prediction:\n",
    "            x_{t-1} = mean(x_t, epsilon_theta, t) + sigma_t * z,  z ~ N(0, I) if t > 0 else 0\n",
    "        \"\"\"\n",
    "        betas_t = self.betas[t].view(-1, 1, 1, 1)\n",
    "        alphas_t = self.alphas[t].view(-1, 1, 1, 1)\n",
    "        alphas_cumprod_t = self.alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "\n",
    "        # Predict epsilon for timestep\n",
    "        eps_theta = self.model(x_t, t, cond)\n",
    "        sqrt_alphas_cumprod_t = torch.sqrt(alphas_cumprod_t)\n",
    "        x0_hat = (x_t - sqrt_one_minus_alphas_cumprod_t * eps_theta) / (\n",
    "            sqrt_alphas_cumprod_t + 1e-8\n",
    "        )\n",
    "\n",
    "        # Compute mean of posterior q(x_{t-1} | x_t, x0_hat)\n",
    "        mean = (1.0 / torch.sqrt(alphas_t)) * (\n",
    "            x_t - (betas_t / (sqrt_one_minus_alphas_cumprod_t + 1e-8)) * eps_theta\n",
    "        )\n",
    "\n",
    "        # add noise except for t == 0\n",
    "        noise = torch.randn_like(x_t)\n",
    "        nonzero_mask = (t > 0).float().view(-1, 1, 1, 1)\n",
    "        x_prev = mean + nonzero_mask * torch.sqrt(betas_t) * noise\n",
    "        return x_prev\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        cond: torch.Tensor,\n",
    "        shape: tuple[int, int, int, int],\n",
    "        x_T: torch.Tensor | None = None,\n",
    "        seed: int | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reverse chain to sample x_0.\n",
    "\n",
    "        Parameters\n",
    "        cond:\n",
    "            Conditioning tensor of shape (B, cond_dim).\n",
    "        shape:\n",
    "            Output sample shape (B, C, H, W).\n",
    "        x_T:\n",
    "            Starting noise. Enables deterministic counterfactuals.\n",
    "        seed:\n",
    "            For reproducible sampling (only used when x_T is None).\n",
    "        \"\"\"\n",
    "        b = shape[0]\n",
    "        cond = cond.to(self.device)\n",
    "\n",
    "        if x_T is not None:\n",
    "            x_t = x_T.to(self.device).clone()\n",
    "        else:\n",
    "            if seed is not None:\n",
    "                g = torch.Generator(device=self.device)\n",
    "                g.manual_seed(seed)\n",
    "                x_t = torch.randn(shape, device=self.device, generator=g)\n",
    "            else:\n",
    "                x_t = torch.randn(shape, device=self.device)\n",
    "\n",
    "        for step in reversed(range(self.timesteps)):\n",
    "            t = torch.full((b,), step, device=self.device, dtype=torch.long)\n",
    "            x_t = self.p_sample(x_t, t, cond)\n",
    "\n",
    "        return x_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_plus_diffusion(config, X_all, C_all):\n",
    "    \"\"\"\n",
    "    Conditional U-Net wrapped in DDPM-style Gaussian diffusion module\n",
    "\n",
    "    Parameters\n",
    "    config:\n",
    "        Experiment configuration dict containing `model` and `diffusion` blocks.\n",
    "    X_all:\n",
    "        Wavelet tensor. Expected shape: (T, N_assets, H_scales)\n",
    "    C_all:\n",
    "        Conditioning tensor\n",
    "    device:\n",
    "        torch.device\n",
    "\n",
    "    Returns\n",
    "    diffusion:\n",
    "        GaussianDiffusion2D wrapping conditional UNet2DCond\n",
    "    \"\"\"\n",
    "    # For energy column\n",
    "    if C_all.ndim == 3:\n",
    "        cond_dim = C_all.shape[2] +1\n",
    "    else:\n",
    "        cond_dim = C_all.shape[1] + 1\n",
    "    \n",
    "    print(f\"Conditioning dimension (reduced): {cond_dim}\")\n",
    "    \n",
    "    m_cfg = config['model']\n",
    "    d_cfg = config['diffusion']\n",
    "\n",
    "    unet = UNET2DCond(\n",
    "        in_channels=m_cfg['input_channels'],\n",
    "        cond_dim=cond_dim,\n",
    "        base_channels=m_cfg['base_channels'],\n",
    "        time_emb_dim=m_cfg['time_emb_dim'],\n",
    "        dropout_rate = 0.1\n",
    "    )\n",
    "\n",
    "    diffusion = GaussianDiffusion2D(\n",
    "        unet,\n",
    "        timesteps=d_cfg['timesteps'],\n",
    "        beta_start=d_cfg['beta_start'],\n",
    "        beta_end=d_cfg['beta_end'],\n",
    "        device=device,\n",
    "        lambda_x0=d_cfg.get('lambda_x0', 0.0),\n",
    "        lambda_spec=d_cfg.get('lambda_spec', 0.0),\n",
    "    ).to(device)\n",
    "\n",
    "    print('Model parameters (in millions): ',\n",
    "          sum(p.numel() for p in diffusion.parameters()) / 1e6)\n",
    "\n",
    "    return diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Diagnostics and Visualization Utilities\n",
    "\n",
    "\n",
    "Helpers evaluate whether generated samples reproduce key \"stylized facts\"\n",
    "of financial returns:\n",
    "\n",
    "- Fat tails (excess kurtosis, tail exceedance probability, survival plots)\n",
    "- Volatility clustering (ACF of r^2 or |r|)\n",
    "- Regime counterfactual tests (same noise seed, different conditioning)\n",
    "\n",
    "All functions are written to be:\n",
    "- batch-safe (work on tensors from DataLoader)\n",
    "- numerically stable\n",
    "- easy to log per epoch\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def clustering_score_from_acf(acf_vals: np.ndarray, lag_lo: int = 1, lag_hi: int = 5) -> float:\n",
    "    \"\"\"\n",
    "    Single scalar summary: average ACF over lags [lag_lo..lag_hi].\n",
    "    \"\"\"\n",
    "    acf_vals = np.asarray(acf_vals)\n",
    "    lag_hi = min(lag_hi, len(acf_vals) - 1)\n",
    "    if lag_hi < lag_lo:\n",
    "        return float(\"nan\")\n",
    "    return float(np.mean(acf_vals[lag_lo:lag_hi + 1]))\n",
    "\n",
    "\n",
    "def max_drawdown(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Approximate max drawdown computed on cumulative sum of return series\n",
    "    Rough diagnostic (not strategy backtest)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    cum = np.cumsum(x)\n",
    "    peak = np.maximum.accumulate(cum)\n",
    "    dd = cum - peak\n",
    "    return float(dd.min())\n",
    "\n",
    "def skewness(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x)\n",
    "    m = x.mean()\n",
    "    s = x.std() + 1e-12\n",
    "    return float(np.mean(((x - m) / s) ** 3))\n",
    "\n",
    "def excess_kurtosis(x: np.ndarray) -> float:\n",
    "    # Gaussian baseline of 0\n",
    "    x = np.asarray(x)\n",
    "    m = x.mean()\n",
    "    s = x.std() + 1e-12\n",
    "    return float(np.mean(((x - m) / s) ** 4) - 3.0)\n",
    "\n",
    "def tail_prob(x: np.ndarray, k: float = 2.0) -> float:\n",
    "    \"\"\"\n",
    "    Tail probabllity: P(|x|) > k * std(x))\n",
    "    In order to declare fat-tailed returns, it must exceed Gaussian expectation.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    s = x.std() + 1e-12\n",
    "    return float(np.mean(np.abs(x) > (k * s)))\n",
    "\n",
    "def wave_stats(x: np.ndarray, name: str = \"\", k: float = 2.0) -> dict:\n",
    "    \"\"\"\n",
    "    Computes small set of diagnostics for return series.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"mean\": float(x.mean()),\n",
    "        \"std\": float(x.std()),\n",
    "        \"mdd\": max_drawdown(x),\n",
    "        \"skew\": skewness(x),\n",
    "        \"ex_kurt\": excess_kurtosis(x),\n",
    "        \"tailP(|r|>kσ)\": tail_prob(x, k=k),\n",
    "    }\n",
    "\n",
    "\n",
    "# Autocorrelation and volatility clustering\n",
    "def acf_1d(x: np.ndarray, nlags: int = 20) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simple autocorrelation function for 1D array x.\n",
    "    with acf[0] = 1.\n",
    "    Dependency free\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    x = x - x.mean()\n",
    "    denom = np.dot(x, x) + 1e-12\n",
    "    out = np.empty(nlags + 1, dtype=np.float64)\n",
    "    out[0] = 1.0\n",
    "    for k in range(1, nlags + 1):\n",
    "        out[k] = np.dot(x[:-k], x[k:]) / denom\n",
    "    return out\n",
    "\n",
    "def vol_cluster_curves(r: np.ndarray, nlags: int = 20, kind: str = \"sq\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Volatility clustering diagnostic\n",
    "    kind='sq' uses r^2\n",
    "    kind='abs' uses |r|\n",
    "    \"\"\"\n",
    "    r = np.asarray(r, dtype=np.float64)\n",
    "    if kind == \"sq\":\n",
    "        v = r ** 2\n",
    "    elif kind == \"abs\":\n",
    "        v = np.abs(r)\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'sq' or 'abs'\")\n",
    "    return acf_1d(v, nlags=nlags)\n",
    "\n",
    "def mean_acf_over_waves(waves: np.ndarray, nlags: int = 20, kind: str = \"sq\") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes mean acf across multiple return window\n",
    "    waves: (N, T)\n",
    "    returns: mean_acf, stderr_acf across waves\n",
    "    \"\"\"\n",
    "    waves = np.asarray(waves)\n",
    "    A = np.stack([vol_cluster_curves(waves[i], nlags=nlags, kind=kind) for i in range(waves.shape[0])], axis=0)\n",
    "    mean = A.mean(axis=0)\n",
    "    stderr = A.std(axis=0) / np.sqrt(A.shape[0] + 1e-12)\n",
    "    return mean, stderr\n",
    "def mean_acf_r2_over_windows(waves: np.ndarray, nlags: int = 20) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Finds mean r-squared over multiple return windows\n",
    "    waves: (N, T) returns windows\n",
    "    returns mean ACF of r^2\n",
    "    \"\"\"\n",
    "    waves = np.asarray(waves, dtype=np.float64)\n",
    "    T = waves.shape[1]\n",
    "    nlags = int(min(nlags, T - 2))\n",
    "    A = np.stack([acf_1d(waves[i]**2, nlags=nlags) for i in range(waves.shape[0])], axis=0)\n",
    "    return A.mean(axis=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_regime_ab_plot(\n",
    "    diffusion,\n",
    "    c_base: torch.Tensor,\n",
    "    epoch: int,\n",
    "    save_dir: str,\n",
    "    shape=(1, 1, 32, 128),\n",
    "    seed: int = 123,\n",
    "    inverse_fn=None,\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Build A/B conditioning with last dim different, for regime comparison\n",
    "    cA = c_base.clone().float()\n",
    "    cB = c_base.clone().float()\n",
    "    cA[-1] = 1.0\n",
    "    cB[-1] = 2.0\n",
    "\n",
    "    cA = cA.view(1, -1).to(diffusion.device)\n",
    "    cB = cB.view(1, -1).to(diffusion.device)\n",
    "\n",
    "    g = torch.Generator(device=diffusion.device)\n",
    "    g.manual_seed(seed)\n",
    "    x_T = torch.randn(shape, device=diffusion.device, generator=g)\n",
    "\n",
    "    xA = diffusion.sample(cA, shape, x_T=x_T)\n",
    "    xB = diffusion.sample(cB, shape, x_T=x_T)\n",
    "\n",
    "    scalA = xA[0, 0].detach().cpu().numpy()\n",
    "    scalB = xB[0, 0].detach().cpu().numpy()\n",
    "\n",
    "    waveA = inverse_fn(scalA) if inverse_fn else None\n",
    "    waveB = inverse_fn(scalB) if inverse_fn else None\n",
    "\n",
    "    nlags = 20\n",
    "    acfA = vol_cluster_curves(waveA, nlags=nlags, kind=\"sq\")\n",
    "    acfB = vol_cluster_curves(waveB, nlags=nlags, kind=\"sq\")\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 7))\n",
    "\n",
    "    # --- Scalogram A ---\n",
    "    ax1 = plt.subplot(2, 2, 1)\n",
    "    im1 = ax1.imshow(scalA, aspect=\"auto\", cmap=\"viridis\")\n",
    "    ax1.set_title(\"Generated Scalogram (Regime A: regime=1.0)\")\n",
    "    ax1.set_xlabel(\"Time\"); ax1.set_ylabel(\"Scale\")\n",
    "    plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
    "\n",
    "    # --- Scalogram B ---\n",
    "    ax2 = plt.subplot(2, 2, 2)\n",
    "    im2 = ax2.imshow(scalB, aspect=\"auto\", cmap=\"viridis\")\n",
    "    ax2.set_title(\"Generated Scalogram (Regime B: regime=2.0)\")\n",
    "    ax2.set_xlabel(\"Time\"); ax2.set_ylabel(\"Scale\")\n",
    "    plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n",
    "\n",
    "    # --- Waves A vs B ---\n",
    "    ax3 = plt.subplot(2, 2, 3)\n",
    "    ax3.plot(waveA, label=\"Wave A (last=1.0)\")\n",
    "    ax3.plot(waveB, label=\"Wave B (last=2.0)\")\n",
    "    ax3.legend()\n",
    "    ax3.set_title(\"Inverse Wave (A vs B) — same noise, different conditioning\")\n",
    "    ax3.set_xlabel(\"Time\"); ax3.set_ylabel(\"Value\")\n",
    "\n",
    "    # --- Volatility clustering proof: ACF of squared returns ---\n",
    "    nlags = 20\n",
    "    acfA = vol_cluster_curves(waveA, nlags=nlags, kind=\"sq\")\n",
    "    acfB = vol_cluster_curves(waveB, nlags=nlags, kind=\"sq\")\n",
    "\n",
    "    ax4 = plt.subplot(2, 2, 4)\n",
    "    lags = np.arange(nlags + 1)\n",
    "    ax4.plot(lags, acfA, marker=\"o\", label=\"ACF(r²) A\")\n",
    "    ax4.plot(lags, acfB, marker=\"o\", label=\"ACF(r²) B\")\n",
    "    ax4.axhline(0.0, linewidth=1)\n",
    "    ax4.set_title(\"Volatility Clustering Diagnostic: ACF of r²\")\n",
    "    ax4.set_xlabel(\"Lag\"); ax4.set_ylabel(\"ACF\")\n",
    "    ax4.legend()\n",
    "    ax4.set_xlim(0, nlags)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(save_dir, f\"epoch_{epoch:04d}_regime_AB.png\")\n",
    "    plt.savefig(out, dpi=200)\n",
    "    plt.close(fig)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_scalogram_comparison(data_matrix, title, plot_path):\n",
    "    \"\"\"\n",
    "    Plots a single (Features, Time) matrix as a 2D scalogram and saves it.\n",
    "    \"\"\"\n",
    "    # Detach from PyTorch and convert to NumPy array for plotting\n",
    "    if torch.is_tensor(data_matrix):\n",
    "        data_matrix = data_matrix.detach().cpu().numpy()\n",
    "\n",
    "    # The data matrix is (Features, Time), which is the correct orientation for imshow\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    im = ax.imshow(data_matrix, aspect='auto', interpolation='none', cmap='viridis')\n",
    "    \n",
    "    # Add colorbar for magnitude reference\n",
    "    plt.colorbar(im, ax=ax, label='Feature Magnitude')\n",
    "    \n",
    "    ax.set_xlabel(f'Time Step in Window (W={data_matrix.shape[1]})')\n",
    "    ax.set_ylabel(f'Wavelet Feature Index (C={data_matrix.shape[0]})')\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot to disk\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close(fig)  # Close to save memory\n",
    "\n",
    "def save_scalogram_plot(data_matrix, title, save_path):\n",
    "    \"\"\"\n",
    "    Saves a scalogram plot.\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    # Plot the data\n",
    "    im = ax.imshow(data_matrix, aspect='auto', interpolation='none', cmap='viridis')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=ax, label='Feature Magnitude')\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel(f'Time Step in Window (W={data_matrix.shape[1]})')\n",
    "    ax.set_ylabel(f'Wavelet Scales (C={data_matrix.shape[0]})')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to file\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def inverse_wavelet_from_scalogram(scalogram_2d, scales=np.arange(1, 33)):\n",
    "    \"\"\"\n",
    "    Pseudo-inverse CWT reconstruction.\n",
    "    Works for Morlet CWT when true icwt is unavailable.\n",
    "\n",
    "    Input:\n",
    "        scalogram_2d: (n_scales, n_times)\n",
    "\n",
    "    Output:\n",
    "        recon: (n_times,) reconstructed return series\n",
    "    \"\"\"\n",
    "    S = np.asarray(scalogram_2d, dtype=np.float64)\n",
    "\n",
    "    # Ensure shape = (n_scales, n_times)\n",
    "    if S.shape[0] != len(scales) and S.shape[1] == len(scales):\n",
    "        S = S.T\n",
    "\n",
    "    if S.shape[0] != len(scales):\n",
    "        raise ValueError(f\"Expected one dimension = n_scales={len(scales)}, got {S.shape}\")\n",
    "\n",
    "    # Scale-weighted sum (standard pseudo-inverse)\n",
    "    weights = 1.0 / np.sqrt(scales)[:, None]   # (n_scales, 1)\n",
    "    recon = np.sum(S * weights, axis=0)\n",
    "\n",
    "    # Normalize energy\n",
    "    recon /= np.sqrt(len(scales))\n",
    "\n",
    "    return recon.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def to_daily_log_returns(price_series):\n",
    "    price_series = np.asarray(price_series)\n",
    "    price_series = np.clip(price_series, 1e-12, None)   # avoid log(0)\n",
    "    return np.diff(np.log(price_series))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Finds log returns\n",
    "def to_daily_log_returns(series_1d, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Computes log returns from positive-values price series.\n",
    "    \"\"\"\n",
    "    s = np.asarray(series_1d, dtype=np.float64)\n",
    "    s = np.clip(s, eps, None)\n",
    "    return np.diff(np.log(s))\n",
    "\n",
    "# --- fat tail plots ---\n",
    "def save_fat_tail_diagnostics(\n",
    "    x_real_batch,\n",
    "    x_fake_batch,\n",
    "    *,\n",
    "    epoch,\n",
    "    save_path,\n",
    "    inverse_fn,\n",
    "    max_rows=64,\n",
    "    use_log_returns=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves a 2x2 diagnostic figure:\n",
    "    - histogram (real vs gen)\n",
    "    - QQ plot (generated vs normal)\n",
    "    - survival plot of |returns|\n",
    "    - volatility clustering: mean ACF of r^2 (real vs gen)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Tensors to numpy\n",
    "    real = x_real_batch.detach().float().cpu().numpy()[:, 0]  # (B, S, T)\n",
    "    fake = x_fake_batch.detach().float().cpu().numpy()[:, 0]\n",
    "\n",
    "    B = min(max_rows, real.shape[0])\n",
    "\n",
    "    real_returns_all = []\n",
    "    fake_returns_all = []\n",
    "    real_waves = []\n",
    "    fake_waves = []\n",
    "\n",
    "\n",
    "    # Waves and returns\n",
    "    for i in range(B):\n",
    "        real_series = inverse_fn(real[i])\n",
    "        fake_series = inverse_fn(fake[i])\n",
    "\n",
    "        if use_log_returns:\n",
    "            real_ret = to_daily_log_returns(real_series)\n",
    "            fake_ret = to_daily_log_returns(fake_series)\n",
    "        else:\n",
    "            real_ret = np.asarray(real_series)\n",
    "            fake_ret = np.asarray(fake_series)\n",
    "\n",
    "        real_ret = real_ret[np.isfinite(real_ret)]\n",
    "        fake_ret = fake_ret[np.isfinite(fake_ret)]\n",
    "\n",
    "        if len(real_ret) < 10 or len(fake_ret) < 10:\n",
    "            continue\n",
    "\n",
    "        real_returns_all.append(real_ret)\n",
    "        fake_returns_all.append(fake_ret)\n",
    "        real_waves.append(real_ret)\n",
    "        fake_waves.append(fake_ret)\n",
    "\n",
    "\n",
    "    # Fat tails\n",
    "    real_r = np.concatenate(real_returns_all)\n",
    "    fake_r = np.concatenate(fake_returns_all)\n",
    "\n",
    "    # Volatility clustering\n",
    "    # Align lengths\n",
    "    min_len = min(map(len, real_waves + fake_waves))\n",
    "    real_waves = np.stack([w[:min_len] for w in real_waves], axis=0)\n",
    "    fake_waves = np.stack([w[:min_len] for w in fake_waves], axis=0)\n",
    "\n",
    "    nlags = min(20, min_len - 2)\n",
    "\n",
    "    real_acf_mean, real_acf_se = mean_acf_over_waves(real_waves, nlags=nlags, kind=\"sq\")\n",
    "    gen_acf_mean,  gen_acf_se  = mean_acf_over_waves(fake_waves,  nlags=nlags, kind=\"sq\")\n",
    "\n",
    "    real_cluster_score = clustering_score_from_acf(real_acf_mean, 1, 5)\n",
    "    gen_cluster_score  = clustering_score_from_acf(gen_acf_mean,  1, 5)\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] Vol clustering score \"\n",
    "        f\"(mean ACF r^2 lags 1..5): real={real_cluster_score:.4f}, \"\n",
    "        f\"gen={gen_cluster_score:.4f}\"\n",
    "    )\n",
    "    mu, sd = fake_r.mean(), fake_r.std(ddof=1)\n",
    "    xs = np.linspace(mu-6*sd, mu+6*sd, 600)\n",
    "\n",
    "\n",
    "\n",
    "    # 5) Plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle(f\"Epoch {epoch} — Fat Tails & Volatility Clustering\", fontsize=14)\n",
    "\n",
    "    #  Histogram ---\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(real_r, bins=80, density=True, alpha=0.6, label=\"Real\")\n",
    "    ax.hist(fake_r, bins=80, density=True, alpha=0.6, label=\"Generated\")\n",
    "    ax.set_title(\"Return Distribution\")\n",
    "    norm_pdf = (1.0 / (1 * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((xs - mu)/sd)**2) if sd > 0 else np.zeros_like(xs)\n",
    "    ax.plot(xs, norm_pdf, linewidth=2, label=\"Normal fit (gen)\")\n",
    "    ax.set_title(\"Histogram (real vs gen) + normal overlay\")\n",
    "    ax.legend()\n",
    "\n",
    "    # QQ plot - generated vs normal\n",
    "    ax = axes[0, 1]\n",
    "    fake_sorted = np.sort(fake_r)\n",
    "    n = len(fake_sorted)\n",
    "    # Normal quantiles via inverse error function approximation \n",
    "    # q = Phi^{-1}((i-0.5)/n)\n",
    "    p = (np.arange(1, n+1) - 0.5) / n\n",
    "    # Approx inverse normal (Acklam-ish simple approx is long); instead do a quick erf^{-1} using numpy via scipy-less:\n",
    "    if hasattr(np, \"erfcinv\"):\n",
    "        z = -np.sqrt(2) * np.erfcinv(2*p)\n",
    "        ax.plot(z, fake_sorted, marker='.', linestyle='none', markersize=3)\n",
    "        # reference line\n",
    "        ax.plot([z.min(), z.max()], [mu + sd*z.min(), mu + sd*z.max()])\n",
    "    else:\n",
    "        ax.plot(fake_sorted)  # fallback\n",
    "    ax.set_title(\"QQ plot vs Normal (generated)\")\n",
    "\n",
    "    # Survival plot \n",
    "    ax = axes[1, 0]\n",
    "    x = np.sort(np.abs(fake_r))\n",
    "    surv = 1.0 - np.arange(1, len(x) + 1) / len(x)\n",
    "    ax.plot(x, surv)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"|r|\")\n",
    "    ax.set_ylabel(\"P(|r| > x)\")\n",
    "    ax.set_title(\"Survival of |Returns| (log y)\")\n",
    "\n",
    "    # Volatility clustering\n",
    "    ax = axes[1, 1]\n",
    "    lags = np.arange(nlags + 1)\n",
    "\n",
    "    ax.plot(lags, real_acf_mean, label=\"Real ACF(r²)\")\n",
    "    ax.fill_between(lags, real_acf_mean - real_acf_se, real_acf_mean + real_acf_se, alpha=0.2)\n",
    "\n",
    "    ax.plot(lags, gen_acf_mean, label=\"Gen ACF(r²)\")\n",
    "    ax.fill_between(lags, gen_acf_mean - gen_acf_se, gen_acf_mean + gen_acf_se, alpha=0.2)\n",
    "\n",
    "    ax.axhline(0.0, linewidth=1)\n",
    "    ax.set_title(\"Volatility Clustering (ACF of r²)\")\n",
    "    ax.set_xlabel(\"Lag\")\n",
    "    ax.set_ylabel(\"ACF\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax.text(\n",
    "        0.02, 0.02,\n",
    "        f\"cluster score (lags1–5):\\n\"\n",
    "        f\"  real = {real_cluster_score:.3f}\\n\"\n",
    "        f\"  gen  = {gen_cluster_score:.3f}\",\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=9,\n",
    "        family=\"monospace\",\n",
    "        va=\"bottom\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_epoch_metrics_png(\n",
    "    metrics_history: list[dict],\n",
    "    *,\n",
    "    epoch: int,\n",
    "    save_dir: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves a metrics-only dashboard PNG per epoch.\n",
    "    Uses the history list (one dict per epoch).\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def series(key, default=np.nan):\n",
    "        xs = []\n",
    "        for m in metrics_history:\n",
    "            v = m.get(key, default)\n",
    "            xs.append(np.nan if v is None else v)\n",
    "        return np.array(xs, dtype=float)\n",
    "\n",
    "    # x-axis epochs\n",
    "    ep = series(\"epoch\")\n",
    "    if np.all(np.isnan(ep)):\n",
    "        ep = np.arange(1, len(metrics_history) + 1)\n",
    "\n",
    "    train_loss = series(\"train_loss\")\n",
    "    val_loss   = series(\"val_loss\")\n",
    "\n",
    "    # distribution alignment\n",
    "    real_std = series(\"real_std\")\n",
    "    gen_std  = series(\"gen_std\")\n",
    "    std_ratio = gen_std / (real_std + 1e-12)\n",
    "\n",
    "    real_exk = series(\"real_exkurt\")\n",
    "    gen_exk  = series(\"gen_exkurt\")\n",
    "    exk_gap  = gen_exk - real_exk\n",
    "\n",
    "    real_sk = series(\"real_skew\")\n",
    "    gen_sk  = series(\"gen_skew\")\n",
    "    skew_gap = gen_sk - real_sk\n",
    "\n",
    "    # tails (abs quantiles)\n",
    "    real_q99 = series(\"real_abs_q990\")\n",
    "    gen_q99  = series(\"gen_abs_q990\")\n",
    "    q99_ratio = gen_q99 / (real_q99 + 1e-12)\n",
    "\n",
    "    real_q995 = series(\"real_abs_q995\")\n",
    "    gen_q995  = series(\"gen_abs_q995\")\n",
    "    q995_ratio = gen_q995 / (real_q995 + 1e-12)\n",
    "\n",
    "    # volatility clustering score\n",
    "    real_cluster = series(\"real_cluster_1_5\")\n",
    "    gen_cluster  = series(\"gen_cluster_1_5\")\n",
    "    cluster_ratio = gen_cluster / (real_cluster + 1e-12)\n",
    "\n",
    "    # Epoch snapshot\n",
    "    m = metrics_history[-1]\n",
    "    def f(key, fmt=\"{:.4g}\", default=\"NA\"):\n",
    "        v = m.get(key, None)\n",
    "        if v is None or (isinstance(v, float) and np.isnan(v)):\n",
    "            return default\n",
    "        try:\n",
    "            return fmt.format(v)\n",
    "        except Exception:\n",
    "            return str(v)\n",
    "\n",
    "    # Figure\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    fig.suptitle(f\"Epoch {epoch} — Metrics Only (Fat Tails & Vol Clustering)\", fontsize=16)\n",
    "\n",
    "    # Loss\n",
    "    ax = plt.subplot(2, 3, 1)\n",
    "    ax.set_title(\"Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Std ratio (gen/real)\n",
    "    ax = plt.subplot(2, 3, 2)\n",
    "    ax.plot(ep, std_ratio)\n",
    "    ax.axhline(1.0, linewidth=1)\n",
    "    ax.set_title(\"Vol level match: std(gen)/std(real)\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    # Tail ratio (q99, q99.5)\n",
    "    ax = plt.subplot(2, 3, 3)\n",
    "    ax.plot(ep, q99_ratio, label=\"|r| q99 ratio\")\n",
    "    ax.plot(ep, q995_ratio, label=\"|r| q99.5 ratio\")\n",
    "    ax.axhline(1.0, linewidth=1)\n",
    "    ax.set_title(\"Tail match (Generated / Real)\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Excess kurtosis gap\n",
    "    ax = plt.subplot(2, 3, 4)\n",
    "    ax.plot(ep, exk_gap)\n",
    "    ax.axhline(0.0, linewidth=1)\n",
    "    ax.set_title(\"Excess kurtosis gap: gen − real\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    # Volatility clustering ratio\n",
    "    ax = plt.subplot(2, 3, 5)\n",
    "    ax.plot(ep, cluster_ratio)\n",
    "    ax.axhline(1.0, linewidth=1)\n",
    "    ax.set_title(\"Vol clustering match: score(gen)/score(real)\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    # Text summary (epoch snapshot)\n",
    "    ax = plt.subplot(2, 3, 6)\n",
    "    ax.axis(\"off\")\n",
    "    text = (\n",
    "        f\"Snapshot @ epoch {epoch}\\n\\n\"\n",
    "        f\"Loss:\\n\"\n",
    "        f\"  train={f('train_loss')}  val={f('val_loss')}\\n\\n\"\n",
    "        f\"Distribution:\\n\"\n",
    "        f\"  real std={f('real_std')}  gen std={f('gen_std')}\\n\"\n",
    "        f\"  real skew={f('real_skew')} gen skew={f('gen_skew')}\\n\"\n",
    "        f\"  real exk={f('real_exkurt')} gen exk={f('gen_exkurt')}\\n\\n\"\n",
    "        f\"Tails (abs):\\n\"\n",
    "        f\"  real q99={f('real_abs_q990')}  gen q99={f('gen_abs_q990')}\\n\"\n",
    "        f\"  real q99.5={f('real_abs_q995')} gen q99.5={f('gen_abs_q995')}\\n\\n\"\n",
    "        f\"Vol clustering:\\n\"\n",
    "        f\"  real score={f('real_cluster_1_5')}  gen score={f('gen_cluster_1_5')}\\n\"\n",
    "    )\n",
    "    ax.text(0.02, 0.98, text, va=\"top\", family=\"monospace\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path = os.path.join(save_dir, f\"metrics_epoch_{epoch:04d}.png\")\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_scalogram_and_waveform(\n",
    "    x_real_batch,\n",
    "    x_fake_batch,\n",
    "    epoch,\n",
    "    save_path,\n",
    "    max_rows=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a 3-across figure per sample:\n",
    "\n",
    "        Real scalogram | Generated scalogram | Reconstructed waveform\n",
    "\n",
    "    x_real_batch, x_fake_batch: tensors of shape (B, 1, n_scales, n_times)\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    if torch.is_tensor(x_real_batch):\n",
    "        x_real = x_real_batch.detach().cpu().numpy()\n",
    "    else:\n",
    "        x_real = x_real_batch\n",
    "\n",
    "    if torch.is_tensor(x_fake_batch):\n",
    "        x_fake = x_fake_batch.detach().cpu().numpy()\n",
    "    else:\n",
    "        x_fake = x_fake_batch\n",
    "\n",
    "    B, _, n_scales, n_times = x_real.shape\n",
    "    rows = min(B, max_rows)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, 3, figsize=(12, 4 * rows))\n",
    "    if rows == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "    for i in range(rows):\n",
    "        real_s = x_real[i, 0]\n",
    "        fake_s = x_fake[i, 0]\n",
    "\n",
    "        # Column 1: real scalogram\n",
    "        ax = axes[i, 0]\n",
    "        im1 = ax.imshow(real_s, aspect='auto', origin='lower', cmap='viridis')\n",
    "        ax.set_title(f\"Real Scalogram #{i}\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Scale\")\n",
    "        fig.colorbar(im1, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Column 2: generated scalogram\n",
    "        ax = axes[i, 1]\n",
    "        im2 = ax.imshow(fake_s, aspect='auto', origin='lower', cmap='viridis')\n",
    "        ax.set_title(f\"Generated Scalogram #{i}\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Scale\")\n",
    "        fig.colorbar(im2, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "        # Column 3: inverse-wavelet time series from generated scalogram\n",
    "        recon = inverse_wavelet_from_scalogram(fake_s)\n",
    "        ax = axes[i, 2]\n",
    "        ax.plot(recon)\n",
    "        ax.set_title(f\"Inverse Wavelet (Gen #{i})\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "\n",
    "    plt.suptitle(f\"Epoch {epoch:04d} – Real vs Generated vs Reconstructed\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "def plot_fat_tail_checks(returns, title=\"Daily returns\"):\n",
    "    r = np.asarray(returns)\n",
    "    r = r[np.isfinite(r)]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "    # 1) Histogram + normal overlay\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(r, bins=80, density=True)\n",
    "    mu, sigma = r.mean(), r.std(ddof=1)\n",
    "    xs = np.linspace(mu - 5*sigma, mu + 5*sigma, 800)\n",
    "    ax.plot(xs, stats.norm.pdf(xs, mu, sigma))\n",
    "    ax.set_title(f\"{title} (hist + normal overlay)\")\n",
    "\n",
    "    # 2) QQ plot vs Normal\n",
    "    ax = axes[0, 1]\n",
    "    stats.probplot(r, dist=\"norm\", plot=ax)\n",
    "    ax.set_title(\"QQ plot vs Normal\")\n",
    "\n",
    "    # 3) Tail survival plot (log y-scale)\n",
    "    ax = axes[1, 0]\n",
    "    x = np.sort(np.abs(r))\n",
    "    surv = 1.0 - (np.arange(1, len(x)+1) / len(x))\n",
    "    ax.plot(x, surv)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(\"Survival of |returns| (log scale)\")\n",
    "    ax.set_xlabel(\"|r|\")\n",
    "    ax.set_ylabel(\"P(|r| > x)\")\n",
    "\n",
    "    # 4) Summary stats\n",
    "    ax = axes[1, 1]\n",
    "    kurt = stats.kurtosis(r, fisher=True, bias=False)  # excess kurtosis\n",
    "    skew = stats.skew(r, bias=False)\n",
    "    ax.axis(\"off\")\n",
    "    ax.text(\n",
    "        0.02, 0.98,\n",
    "        f\"n = {len(r)}\\nmean = {mu:.4g}\\nstd = {sigma:.4g}\\nskew = {skew:.4g}\\nexcess kurtosis = {kurt:.4g}\",\n",
    "        va=\"top\"\n",
    "    )\n",
    "    ax.set_title(\"Moments (fat tails → high excess kurtosis)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_stats(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute per-channel mean and variance over batch and time.\n",
    "    x: (B, C, T) or (B, D)\n",
    "\n",
    "    Returns:\n",
    "        mean \n",
    "        var\n",
    "    \"\"\"\n",
    "    if x.ndim == 3:\n",
    "        x_flat = x.permute(0, 2, 1).reshape(-1, x.size(1))\n",
    "    else:\n",
    "        x_flat = x\n",
    "    mean = x_flat.mean(dim=0)\n",
    "    var = x_flat.var(dim=0)\n",
    "    return mean, var\n",
    "\n",
    "def avg_pairwise_dist(x: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Average pairwise Euclidean distance between samples in a batch.\n",
    "\n",
    "    Parameters\n",
    "    x:\n",
    "        Tensor of shape (B, ). Will be flattened per sample.\n",
    "    max_pairs:\n",
    "        Computes distance on a random subset of pairs to avoid large complexity.\n",
    "\n",
    "    Returns\n",
    "    float\n",
    "        Mean pairwise distance.\n",
    "    \"\"\"\n",
    "    B = x.size(0)\n",
    "    x_flat = x.view(B, -1)\n",
    "    diff = x_flat.unsqueeze(1) - x_flat.unsqueeze(0)\n",
    "    dist_mat = (diff ** 2).mean(dim=-1).sqrt()\n",
    "    mask = torch.triu(torch.ones_like(dist_mat), diagonal=1) > 0\n",
    "    return dist_mat[mask].mean().item()\n",
    "\n",
    "def rbf_kernel(x: torch.Tensor, y: torch.Tensor, sigma: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    RBF kernel K(x, y) = exp(-||x-y||^2 / (2*sigma^2)).\n",
    "\n",
    "    x : (N, D)\n",
    "    y : (M, D)\n",
    "    returns : (N, M)\n",
    "    \"\"\"\n",
    "    x = x.unsqueeze(1)\n",
    "    y = y.unsqueeze(0)\n",
    "    dist2 = ((x - y) ** 2).sum(-1)\n",
    "    return torch.exp(-dist2 / (2 * sigma ** 2))\n",
    "\n",
    "def mmd_flat(x: torch.Tensor, y: torch.Tensor, sigma: float = 10.0) -> float:\n",
    "    \"\"\"\n",
    "    Maximum Mean Discrepancy (MMD) between two sample sets using RBF kernel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y:\n",
    "        Tensors of shape (B, ) flattened to (B, D).\n",
    "    sigma:\n",
    "        Kernel bandwidth\n",
    "\n",
    "    Returns\n",
    "    float\n",
    "        MMD estimate (scalar).\n",
    "    \"\"\"\n",
    "    x_flat = x.view(x.size(0), -1)\n",
    "    y_flat = y.view(y.size(0), -1)\n",
    "    Kxx = rbf_kernel(x_flat, x_flat, sigma).mean()\n",
    "    Kyy = rbf_kernel(y_flat, y_flat, sigma).mean()\n",
    "    Kxy = rbf_kernel(x_flat, y_flat, sigma).mean()\n",
    "    return (Kxx + Kyy - 2 * Kxy).item()\n",
    "\n",
    "def compute_fid(real_features, fake_features):\n",
    "    \"\"\"Frechet Inception Distance approximation\"\"\"\n",
    "    mu_real, sigma_real = torch.mean(real_features, dim=0), torch.cov(real_features.T)\n",
    "    mu_fake, sigma_fake = torch.mean(fake_features, dim=0), torch.cov(fake_features.T)\n",
    "    \n",
    "    diff = mu_real - mu_fake\n",
    "    covmean = torch.sqrt(sigma_real @ sigma_fake)\n",
    "    \n",
    "    if torch.iscomplex(covmean):\n",
    "        covmean = covmean.real\n",
    "        \n",
    "    fid = diff.dot(diff) + torch.trace(sigma_real + sigma_fake - 2 * covmean)\n",
    "    return fid.item()\n",
    "\n",
    "def compute_metrics(real_batch, fake_batch):\n",
    "    \"\"\"Computes comprehensive evaluation metrics\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Basic statistics\n",
    "        mu_real, var_real = basic_stats(real_batch)\n",
    "        mu_fake, var_fake = basic_stats(fake_batch)\n",
    "        \n",
    "        mean_diff = (mu_real - mu_fake).abs().mean().item()\n",
    "        var_diff = (var_real - var_fake).abs().mean().item()\n",
    "        \n",
    "        # MMD\n",
    "        mmd_val = mmd_flat(real_batch, fake_batch, sigma=10.0)\n",
    "        \n",
    "        # Diversity metrics\n",
    "        real_flat = real_batch.view(real_batch.size(0), -1)\n",
    "        fake_flat = fake_batch.view(fake_batch.size(0), -1)\n",
    "        \n",
    "        real_diversity = real_flat.std(dim=0).mean().item()\n",
    "        fake_diversity = fake_flat.std(dim=0).mean().item()\n",
    "        diversity_ratio = fake_diversity / max(real_diversity, 1e-8)\n",
    "        \n",
    "        # Energy statistics\n",
    "        real_energy = (real_batch ** 2).mean().item()\n",
    "        fake_energy = (fake_batch ** 2).mean().item()\n",
    "        energy_diff = abs(real_energy - fake_energy) / max(real_energy, 1e-8)\n",
    "        \n",
    "        # Spectrum correlation (frequency domain)\n",
    "        real_fft = torch.fft.rfft(real_batch, dim=-1)\n",
    "        fake_fft = torch.fft.rfft(fake_batch, dim=-1)\n",
    "        \n",
    "        real_mag = torch.abs(real_fft)\n",
    "        fake_mag = torch.abs(fake_fft)\n",
    "        \n",
    "        # Flatten for correlation\n",
    "        real_mag_flat = real_mag.view(-1)\n",
    "        fake_mag_flat = fake_mag.view(-1)\n",
    "        \n",
    "        # Handle constant tensors\n",
    "        if real_mag_flat.std() > 1e-8 and fake_mag_flat.std() > 1e-8:\n",
    "            spectrum_corr = torch.corrcoef(\n",
    "                torch.stack([real_mag_flat, fake_mag_flat])\n",
    "            )[0, 1].item()\n",
    "        else:\n",
    "            spectrum_corr = 0.0\n",
    "    \n",
    "    return {\n",
    "        'mean_diff': mean_diff,\n",
    "        'var_diff': var_diff,\n",
    "        'mmd': mmd_val,\n",
    "        'real_diversity': real_diversity,\n",
    "        'fake_diversity': fake_diversity,\n",
    "        'diversity_ratio': diversity_ratio,\n",
    "        'real_energy': real_energy,\n",
    "        'fake_energy': fake_energy,\n",
    "        'energy_diff': energy_diff,\n",
    "        'spectrum_correlation': spectrum_corr,\n",
    "        'combined_score': mean_diff + 0.5 * var_diff + 0.1 * mmd_val + 0.2 * energy_diff\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with Visualization - MODIFIED to save EVERY epoch\n",
    "\n",
    "\n",
    "\n",
    "def train_ddpm_with_plots(config, diffusion, train_loader, val_loader, experiment_name=\"exp1\"):\n",
    "    \"\"\"\n",
    "    Trains a conditional DDPM and save:\n",
    "      1) Checkpoints (best + periodic)\n",
    "      2) Qualitative plots every epoch (real vs generated scalograms)\n",
    "      3) Finance diagnostics every epoch:\n",
    "         - inverse-wavelet reconstructed \"return\" waves\n",
    "         - volatility clustering ACF(r^2)\n",
    "         - fat-tail diagnostics (hist/QQ/survival)\n",
    "      4) Metric history (loss, MMD, moment diffs, learning rate, grad norms)\n",
    "\n",
    "\n",
    "    Inputs\n",
    "    config : dict\n",
    "        Needs:\n",
    "          - config['training']: learning_rate, weight_decay, num_epochs, max_grad_norm, etc.\n",
    "          - config['paths']: checkpoint_dir\n",
    "          - optionally config['data']: x_mean, x_std for de-normalization\n",
    "    diffusion : GaussianDiffusion2D\n",
    "        Diffusion wrapper with .p_losses() (training loss) and .sample() (reverse process).\n",
    "    train_loader / val_loader : DataLoader\n",
    "        Yield tuples (x_batch, c_batch), where:\n",
    "          x_batch: (B, 1, H, W) scalograms\n",
    "          c_batch: (B, cond_dim) conditioning vectors\n",
    "    experiment_name : str\n",
    "        Used to separate output folders for different runs.\n",
    "\n",
    "    Returns\n",
    "    checkpoint_path : str\n",
    "        Path to the \"best\" checkpoint (NOTE: current code overwrites every epoch).\n",
    "    best_val_loss : float\n",
    "    history : dict\n",
    "        Contains loss curves and simple distribution metrics.\n",
    "    \"\"\"\n",
    "    train_cfg = config['training']\n",
    "    paths = config['paths']\n",
    "    \n",
    "    \n",
    "    # Create directories\n",
    "    checkpoint_dir = paths['checkpoint_dir']\n",
    "    plot_dir = '/home/dsranelli/bigproject/artifacts_all/best_samples'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories for this experiment\n",
    "    exp_plot_dir = os.path.join(plot_dir, experiment_name)\n",
    "    os.makedirs(exp_plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Create epoch-specific subdirectory\n",
    "    epochs_dir = os.path.join(exp_plot_dir, \"epoch_plots_with_wave\")\n",
    "    os.makedirs(epochs_dir, exist_ok=True)\n",
    "\n",
    "    # Optimizer and learning rate scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        diffusion.parameters(),\n",
    "        lr=config['training']['learning_rate'],\n",
    "        weight_decay=config['training']['weight_decay']\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config['training']['num_epochs'],\n",
    "        eta_min=config['training']['learning_rate'] * 0.1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_epochs = train_cfg['num_epochs']\n",
    "    max_grad_norm = train_cfg['max_grad_norm']\n",
    "    early_stop_pat = train_cfg.get('early_stop_patience', None)\n",
    "    \n",
    "    \n",
    "    # Training history for plotting\n",
    "    history = {\n",
    "        'epochs': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'mean_diff': [],\n",
    "        'var_diff': [],\n",
    "        'mmd': [],\n",
    "        'learning_rates': [],\n",
    "        'grad_norms': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nStarting training for experiment: {experiment_name}\")\n",
    "    print(f\"Checkpoints saved to: {checkpoint_dir}\")\n",
    "    print(f\"Epoch plots saved to: {epochs_dir}\")\n",
    "\n",
    "    ema_decay = 0.995\n",
    "    ema_diffusion = copy.deepcopy(diffusion).to(diffusion.device)\n",
    "    for p in ema_diffusion.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    def update_ema(ema_model, model, decay):\n",
    "        \"\"\"\n",
    "        EMA update: ema = decay * ema + (1 - decay) * model\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (k, v), (_, v_model) in zip(ema_model.state_dict().items(),\n",
    "                                            model.state_dict().items()):\n",
    "                v.copy_(decay * v + (1.0 - decay) * v_model)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories for this experiment\n",
    "    exp_plot_dir = os.path.join(plot_dir, experiment_name)\n",
    "    os.makedirs(exp_plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Create epoch-specific subdirectory\n",
    "    epochs_dir = os.path.join(exp_plot_dir, \"epoch_plots_with_wave\")\n",
    "    os.makedirs(epochs_dir, exist_ok=True)\n",
    "    # Directory for reverse-wavelet visualizations:\n",
    "    waves_dir = os.path.join(exp_plot_dir, \"epoch_plots1000\")\n",
    "    os.makedirs(waves_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "    \n",
    "        diffusion.train()\n",
    "        train_loss_accum = 0.0\n",
    "        n_train = 0\n",
    "        grad_norms = []\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [train]\")\n",
    "        for step, (x_batch, c_batch) in enumerate(train_pbar):\n",
    "            x_batch = x_batch.to(diffusion.device)\n",
    "            c_batch = c_batch.to(diffusion.device)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "            # Training objective\n",
    "            loss = diffusion.p_losses(x_batch, c_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            max_grad = train_cfg.get('max_grad_norm', 1.0)\n",
    "            if max_grad is not None:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    diffusion.parameters(),  # <-- main model, not EMA\n",
    "                    max_norm=max_grad\n",
    "                ).item()\n",
    "            else:\n",
    "                grad_norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm() for p in diffusion.parameters()\n",
    "                        if p.grad is not None\n",
    "                    ])\n",
    "                ).item()\n",
    "            optimizer.step()\n",
    "            grad_norms.append(grad_norm)\n",
    "\n",
    "            # Update EMA weights after each optimizer step\n",
    "            update_ema(ema_diffusion, diffusion, ema_decay)\n",
    "\n",
    "            # Update statistics\n",
    "            bs = x_batch.size(0)\n",
    "            train_loss_accum += loss.item() * bs\n",
    "            n_train += bs\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "            train_loss = train_loss_accum / max(n_train, 1)\n",
    "            avg_grad_norm = np.mean(grad_norms) if grad_norms else 0\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Step LR schedule once per epoch\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['grad_norms'].append(avg_grad_norm)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Validation\n",
    "        diffusion.eval()\n",
    "        val_loss_accum = 0.0\n",
    "        n_val = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [val]\")\n",
    "        with torch.no_grad():\n",
    "            for x_batch, c_batch in val_pbar:\n",
    "                x_batch = x_batch.to(diffusion.device)\n",
    "                c_batch = c_batch.to(diffusion.device)\n",
    "                \n",
    "                loss = diffusion.p_losses(x_batch, c_batch)\n",
    "                bs = x_batch.size(0)\n",
    "                val_loss_accum += loss.item() * bs\n",
    "                n_val += bs\n",
    "                \n",
    "                val_pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        val_loss = val_loss_accum / max(n_val, 1)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['epochs'].append(epoch + 1)\n",
    "\n",
    "        \n",
    "        # Sample generation and metrics\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Get validation batch for visualization\n",
    "                x_val_batch, c_val_batch = next(iter(val_loader))\n",
    "                x_val_batch = x_val_batch[:64].to(diffusion.device)\n",
    "                c_val_batch = c_val_batch[:64].to(diffusion.device)\n",
    "                \n",
    "                # Generate samples using ema\n",
    "                x_fake_batch = ema_diffusion.sample(c_val_batch, x_val_batch.shape)\n",
    "                # De-normalize for plotting / metric checks\n",
    "                if 'x_mean' in config['data']:\n",
    "                    mu = config['data']['x_mean']\n",
    "                    sigma = config['data']['x_std']\n",
    "                    x_val_batch_plot = x_val_batch * sigma + mu\n",
    "                    x_fake_batch_plot = x_fake_batch * sigma + mu\n",
    "                else:\n",
    "                    x_val_batch_plot = x_val_batch\n",
    "                    x_fake_batch_plot = x_fake_batch\n",
    "                \n",
    "                # Distribution checks\n",
    "                mu_real, var_real = basic_stats(x_val_batch)\n",
    "                mu_fake, var_fake = basic_stats(x_fake_batch)\n",
    "                mean_diff = (mu_real - mu_fake).abs().mean().item()\n",
    "                var_diff = (var_real - var_fake).abs().mean().item()\n",
    "                mmd_val = mmd_flat(x_val_batch, x_fake_batch, sigma=10.0)\n",
    "                \n",
    "                history['mean_diff'].append(mean_diff)\n",
    "                history['var_diff'].append(var_diff)\n",
    "                history['mmd'].append(mmd_val)\n",
    "                \n",
    "                # Convert to numpy for plotting\n",
    "                real_np = x_val_batch.cpu().numpy()\n",
    "                fake_np = x_fake_batch.cpu().numpy()\n",
    "\n",
    "            # Edge case for empty validation loader\n",
    "            except StopIteration:\n",
    "                mean_diff = var_diff = mmd_val = float('nan')\n",
    "                history['mean_diff'].append(float('nan'))\n",
    "                history['var_diff'].append(float('nan'))\n",
    "                history['mmd'].append(float('nan'))\n",
    "                real_np = fake_np = None\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Print Progress\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Time: {epoch_time:.1f}s\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Train Loss: {train_loss:.6f}\")\n",
    "        print(f\"Val Loss:   {val_loss:.6f}\")\n",
    "        print(f\"LR:         {current_lr:.2e}\")\n",
    "        print(f\"Grad Norm:  {avg_grad_norm:.2f}\")\n",
    "        \n",
    "        if not np.isnan(mean_diff):\n",
    "            print(f\"{'='*60}\")\n",
    "            print(\"Generation Metrics:\")\n",
    "            print(f\"  Mean Diff:  {mean_diff:.6f}\")\n",
    "            print(f\"  Var Diff:   {var_diff:.6f}\")\n",
    "            print(f\"  MMD:        {mmd_val:.6f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Save Visualizations for each Epoch\n",
    "        if real_np is not None and fake_np is not None:\n",
    "            # Create epoch directory\n",
    "            epoch_dir = os.path.join(epochs_dir, f\"epoch_{epoch+1:04d}\")\n",
    "            os.makedirs(epoch_dir, exist_ok=True)\n",
    "            \n",
    "            # Save individual samples\n",
    "            for i in range(min(4, real_np.shape[0])):\n",
    "                # Real sample\n",
    "                real_data = real_np[i, 0]\n",
    "                real_path = os.path.join(epoch_dir, f\"sample_{i}_real.png\")\n",
    "                save_scalogram_plot(\n",
    "                    real_data,\n",
    "                    f'Real Scalogram - Epoch {epoch+1}, Sample {i} (Loss: {val_loss:.4f})',\n",
    "                    real_path\n",
    "                )\n",
    "                \n",
    "                # Generated sample\n",
    "                fake_data = fake_np[i, 0]\n",
    "                fake_path = os.path.join(epoch_dir, f\"sample_{i}_generated.png\")\n",
    "                save_scalogram_plot(\n",
    "                    fake_data,\n",
    "                    f'Generated Scalogram - Epoch {epoch+1}, Sample {i} (Loss: {val_loss:.4f})',\n",
    "                    fake_path\n",
    "                )\n",
    "            \n",
    "            # Save comparison grid\n",
    "            fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "            fig.suptitle(f'Epoch {epoch+1} - Val Loss: {val_loss:.4f}', fontsize=16)\n",
    "            vmin, vmax = -3, 3   # adjust if needed\n",
    "            for i in range(min(4, real_np.shape[0])):\n",
    "                im1 = axes[0, i].imshow(\n",
    "                    real_np[i, 0],\n",
    "                    aspect='auto',\n",
    "                    cmap='viridis',\n",
    "                    vmin=vmin,\n",
    "                    vmax=vmax\n",
    "                )\n",
    "                axes[0, i].set_title(f'Real Sample {i}')\n",
    "                axes[0, i].set_ylabel('Scales')\n",
    "                axes[0, i].set_xlabel('Time')\n",
    "                plt.colorbar(im1, ax=axes[0, i], shrink=0.7)\n",
    "\n",
    "                # Fake\n",
    "                im2 = axes[1, i].imshow(\n",
    "                    fake_np[i, 0],\n",
    "                    aspect='auto',\n",
    "                    cmap='viridis',\n",
    "                    vmin=vmin,\n",
    "                    vmax=vmax\n",
    "                )\n",
    "                axes[1, i].set_title(f'Generated Sample {i}')\n",
    "                axes[1, i].set_ylabel('Scales')\n",
    "                axes[1, i].set_xlabel('Time')\n",
    "                plt.colorbar(im2, ax=axes[1, i], shrink=0.7)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            grid_path = os.path.join(epoch_dir, \"comparison_grid.png\")\n",
    "            plt.savefig(grid_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            \n",
    "            \n",
    "            # Save 3-across plot: [Real | Generated | Inverse Wavelet]\n",
    "            wave_plot_path = os.path.join(\n",
    "                waves_dir,\n",
    "                f\"epoch_{epoch+1:04d}_waves.png\"\n",
    "            )\n",
    "\n",
    "            # Fixed base conditioning vector once (so only last column changes)\n",
    "            if epoch == 0:\n",
    "                c_base_fixed = c_val_batch[0].detach().cpu()\n",
    "                print(\"Cached c_base_fixed. Last element (before override):\", float(c_base_fixed[-1]))\n",
    "\n",
    "            # Save deterministic Regime A/B plot each epoch\n",
    "            ab_path = save_regime_ab_plot(\n",
    "                diffusion=ema_diffusion,\n",
    "                c_base=c_base_fixed,\n",
    "                epoch=epoch+1,\n",
    "                save_dir=waves_dir,\n",
    "                shape=(1, 1, 32, 128),\n",
    "                seed=123,\n",
    "                inverse_fn=inverse_wavelet_from_scalogram,\n",
    "            )\n",
    "            print(\"Saved regime A/B plot:\", ab_path)\n",
    "\n",
    "            plot_scalogram_and_waveform(\n",
    "                x_real_batch=x_val_batch,\n",
    "                x_fake_batch=x_fake_batch,\n",
    "                epoch=epoch+1,\n",
    "                save_path=wave_plot_path,\n",
    "                max_rows=3,   # up to 3 samples per epoch\n",
    "            )\n",
    "            # Save fat-tail diagnostics right after wave plots\n",
    "            fat_tail_path = os.path.join(waves_dir, f\"epoch_{epoch+1:04d}_fat_tails.png\")\n",
    "\n",
    "            save_fat_tail_diagnostics(\n",
    "                x_real_batch=x_val_batch_plot if 'x_mean' in config['data'] else x_val_batch,\n",
    "                x_fake_batch=x_fake_batch_plot if 'x_mean' in config['data'] else x_fake_batch,\n",
    "                epoch=epoch+1,\n",
    "                save_path=fat_tail_path,\n",
    "                inverse_fn=inverse_wavelet_from_scalogram,\n",
    "                max_rows=20000,\n",
    "                use_log_returns=False,\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Metrics history\n",
    "            if \"metrics_history\" not in locals():\n",
    "                metrics_history = []\n",
    "\n",
    "            # Batch selection\n",
    "            x_real_for_metrics = x_val_batch_plot if 'x_mean' in config['data'] else x_val_batch\n",
    "            x_fake_for_metrics = x_fake_batch_plot\n",
    "\n",
    "            # Convert to numpy scalograms: (B,1,S,T) -> (B,S,T)\n",
    "            real_np = x_real_for_metrics.detach().float().cpu().numpy()[:, 0]\n",
    "            fake_np = x_fake_for_metrics.detach().float().cpu().numpy()[:, 0]\n",
    "\n",
    "            B = min(128, real_np.shape[0])\n",
    "\n",
    "            # Reconstruct \"returns waves\" per sample using inverse\n",
    "            real_waves = []\n",
    "            gen_waves  = []\n",
    "            for i in range(B):\n",
    "                r = inverse_wavelet_from_scalogram(real_np[i])\n",
    "                g = inverse_wavelet_from_scalogram(fake_np[i])\n",
    "\n",
    "                r = np.asarray(r, dtype=np.float64)\n",
    "                g = np.asarray(g, dtype=np.float64)\n",
    "\n",
    "                r = r[np.isfinite(r)]\n",
    "                g = g[np.isfinite(g)]\n",
    "                if len(r) < 20 or len(g) < 20:\n",
    "                    continue\n",
    "\n",
    "                real_waves.append(r)\n",
    "                gen_waves.append(g)\n",
    "\n",
    "            # align lengths for ACF\n",
    "            min_len = min(map(len, real_waves + gen_waves))\n",
    "            real_waves = np.stack([w[:min_len] for w in real_waves], axis=0)  # (N,T)\n",
    "            gen_waves  = np.stack([w[:min_len] for w in gen_waves], axis=0)\n",
    "\n",
    "            # flatten for distribution metrics\n",
    "            real_r = real_waves.reshape(-1)\n",
    "            gen_r  = gen_waves.reshape(-1)\n",
    "\n",
    "            # compute clustering scores\n",
    "            real_acf = mean_acf_r2_over_windows(real_waves, nlags=20)\n",
    "            gen_acf  = mean_acf_r2_over_windows(gen_waves,  nlags=20)\n",
    "\n",
    "            row = {\n",
    "                \"epoch\": int(epoch),\n",
    "\n",
    "\n",
    "                \"real_std\": float(np.std(real_r, ddof=1)),\n",
    "                \"gen_std\": float(np.std(gen_r,  ddof=1)),\n",
    "\n",
    "                \"real_skew\": float(skewness(real_r)),\n",
    "                \"gen_skew\": float(skewness(gen_r)),\n",
    "\n",
    "                \"real_exkurt\": float(excess_kurtosis(real_r)),\n",
    "                \"gen_exkurt\": float(excess_kurtosis(gen_r)),\n",
    "\n",
    "                \"real_abs_q990\": float(np.quantile(np.abs(real_r), 0.990)),\n",
    "                \"gen_abs_q990\": float(np.quantile(np.abs(gen_r),  0.990)),\n",
    "\n",
    "                \"real_abs_q995\": float(np.quantile(np.abs(real_r), 0.995)),\n",
    "                \"gen_abs_q995\": float(np.quantile(np.abs(gen_r),  0.995)),\n",
    "\n",
    "                \"real_cluster_1_5\": float(clustering_score_from_acf(real_acf, 1, 5)),\n",
    "                \"gen_cluster_1_5\": float(clustering_score_from_acf(gen_acf,  1, 5)),\n",
    "            }\n",
    "\n",
    "            metrics_history.append(row)\n",
    "\n",
    "            # Save PNG dashboard for this epoch\n",
    "            save_epoch_metrics_png(\n",
    "                metrics_history,\n",
    "                epoch=int(epoch),\n",
    "                save_dir=\"/home/dsranelli/bigproject/artifacts_all/best_samples/full_training/epoch_plots1000/metrics_png\"\n",
    "            )\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            # Save metrics plot for this epoch\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "            \n",
    "            # Loss plot (up to current epoch)\n",
    "            epochs_so_far = list(range(1, len(history['epochs']) + 1))\n",
    "            axes[0, 0].plot(epochs_so_far, history['train_loss'], 'b-', label='Train', marker='o', markersize=3)\n",
    "            axes[0, 0].plot(epochs_so_far, history['val_loss'], 'r-', label='Val', marker='s', markersize=3)\n",
    "            axes[0, 0].axvline(x=epoch+1, color='g', linestyle='--', alpha=0.5)\n",
    "            axes[0, 0].set_xlabel('Epoch')\n",
    "            axes[0, 0].set_ylabel('Loss')\n",
    "            axes[0, 0].set_title('Training Progress')\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Learning rate\n",
    "            axes[0, 1].plot(epochs_so_far, history['learning_rates'], 'g-', marker='^', markersize=3)\n",
    "            axes[0, 1].axvline(x=epoch+1, color='g', linestyle='--', alpha=0.5)\n",
    "            axes[0, 1].set_xlabel('Epoch')\n",
    "            axes[0, 1].set_ylabel('Learning Rate')\n",
    "            axes[0, 1].set_title('Learning Rate Schedule')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            axes[0, 1].set_yscale('log')\n",
    "            \n",
    "            # Gradient norms\n",
    "            axes[0, 2].plot(epochs_so_far, history['grad_norms'], 'm-', marker='d', markersize=3)\n",
    "            axes[0, 2].axvline(x=epoch+1, color='g', linestyle='--', alpha=0.5)\n",
    "            axes[0, 2].set_xlabel('Epoch')\n",
    "            axes[0, 2].set_ylabel('Gradient Norm')\n",
    "            axes[0, 2].set_title('Gradient Norms')\n",
    "            axes[0, 2].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Mean difference\n",
    "            if not all(np.isnan(history['mean_diff'])):\n",
    "                axes[1, 0].plot(epochs_so_far, history['mean_diff'], 'c-', marker='o', markersize=3)\n",
    "                axes[1, 0].axvline(x=epoch+1, color='g', linestyle='--', alpha=0.5)\n",
    "                axes[1, 0].set_xlabel('Epoch')\n",
    "                axes[1, 0].set_ylabel('Mean Difference')\n",
    "                axes[1, 0].set_title('Mean Distribution Difference')\n",
    "                axes[1, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Variance difference\n",
    "            if not all(np.isnan(history['var_diff'])):\n",
    "                axes[1, 1].plot(epochs_so_far, history['var_diff'], 'orange', marker='s', markersize=3)\n",
    "                axes[1, 1].axvline(x=epoch+1, color='g', linestyle='--', alpha=0.5)\n",
    "                axes[1, 1].set_xlabel('Epoch')\n",
    "                axes[1, 1].set_ylabel('Variance Difference')\n",
    "                axes[1, 1].set_title('Variance Distribution Difference')\n",
    "                axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # MMD\n",
    "            if not all(np.isnan(history['mmd'])):\n",
    "                axes[1, 2].plot(epochs_so_far, history['mmd'], 'purple', marker='^', markersize=3)\n",
    "                axes[1, 2].axvline(x=epoch+1, color='g', linestyle='--', alpha=0.5)\n",
    "                axes[1, 2].set_xlabel('Epoch')\n",
    "                axes[1, 2].set_ylabel('MMD')\n",
    "                axes[1, 2].set_title('Maximum Mean Discrepancy')\n",
    "                axes[1, 2].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.suptitle(f'Training Progress - Epoch {epoch+1}', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            metrics_path = os.path.join(epoch_dir, \"training_progress.png\")\n",
    "            plt.savefig(metrics_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Save epoch summary as JSON\n",
    "            epoch_summary = {\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': float(train_loss),\n",
    "                'val_loss': float(val_loss),\n",
    "                'learning_rate': float(current_lr),\n",
    "                'grad_norm': float(avg_grad_norm),\n",
    "                'mean_diff': float(mean_diff) if not np.isnan(mean_diff) else None,\n",
    "                'var_diff': float(var_diff) if not np.isnan(var_diff) else None,\n",
    "                'mmd': float(mmd_val) if not np.isnan(mmd_val) else None,\n",
    "                'epoch_time': float(epoch_time),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            with open(os.path.join(epoch_dir, \"summary.json\"), 'w') as f:\n",
    "                json.dump(epoch_summary, f, indent=2)\n",
    "            \n",
    "            # Save simple text summary\n",
    "            with open(os.path.join(epoch_dir, \"summary.txt\"), 'w') as f:\n",
    "                f.write(f\"Epoch {epoch+1} Summary\\n\")\n",
    "                f.write(f\"=====================\\n\\n\")\n",
    "                f.write(f\"Train Loss: {train_loss:.6f}\\n\")\n",
    "                f.write(f\"Val Loss:   {val_loss:.6f}\\n\")\n",
    "                f.write(f\"Learning Rate: {current_lr:.2e}\\n\")\n",
    "                f.write(f\"Gradient Norm: {avg_grad_norm:.2f}\\n\")\n",
    "                if not np.isnan(mean_diff):\n",
    "                    f.write(f\"Mean Difference: {mean_diff:.6f}\\n\")\n",
    "                    f.write(f\"Variance Difference: {var_diff:.6f}\\n\")\n",
    "                    f.write(f\"MMD: {mmd_val:.6f}\\n\")\n",
    "                f.write(f\"Epoch Time: {epoch_time:.1f}s\\n\")\n",
    "                f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "            \n",
    "            print(f\"  ✓ Visualizations saved to: {epoch_dir}\")\n",
    "        \n",
    "            \n",
    "        # Save best model checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"{experiment_name}_best.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': diffusion.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss,\n",
    "            'config': config,\n",
    "            'history': history\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Save periodic checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            periodic_path = os.path.join(checkpoint_dir, f\"{experiment_name}_epoch_{epoch+1:04d}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': diffusion.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'train_loss': train_loss,\n",
    "                'config': config,\n",
    "                'history': history\n",
    "            }, periodic_path)\n",
    "            print(f\"  ✓ Periodic checkpoint saved: {periodic_path}\")\n",
    "\n",
    "        \n",
    "        # Save intermediate history every epoch\n",
    "        history_path = os.path.join(exp_plot_dir, \"training_history.npy\")\n",
    "        np.save(history_path, history)\n",
    "    \n",
    "    # Final Summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Best epoch: {best_epoch}\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "    print(f\"Total epochs trained: {epoch + 1}\")\n",
    "    print(f\"\\nCheckpoints saved to: {checkpoint_dir}\")\n",
    "    print(f\"Epoch visualizations saved to: {epochs_dir}\")\n",
    "    print(f\"Training history saved to: {os.path.join(exp_plot_dir, 'training_history.npy')}\")\n",
    "    \n",
    "    # Creates final summary plot\n",
    "    if len(history['epochs']) > 0:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Loss curves\n",
    "        axes[0, 0].plot(history['epochs'], history['train_loss'], 'b-', label='Train', alpha=0.7)\n",
    "        axes[0, 0].plot(history['epochs'], history['val_loss'], 'r-', label='Val', alpha=0.7)\n",
    "        axes[0, 0].axvline(x=best_epoch, color='g', linestyle='--', label=f'Best (epoch {best_epoch})')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Final Training Curves')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate\n",
    "        axes[0, 1].plot(history['epochs'], history['learning_rates'], 'g-')\n",
    "        axes[0, 1].axvline(x=best_epoch, color='g', linestyle='--')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Learning Rate')\n",
    "        axes[0, 1].set_title('Learning Rate Schedule')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].set_yscale('log')\n",
    "        \n",
    "        # Metrics\n",
    "        if not all(np.isnan(history['mmd'])):\n",
    "            axes[1, 0].plot(history['epochs'], history['mmd'], 'purple', label='MMD')\n",
    "            axes[1, 0].axvline(x=best_epoch, color='g', linestyle='--')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('MMD')\n",
    "            axes[1, 0].set_title('Distribution Distance (MMD)')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        if not all(np.isnan(history['mean_diff'])):\n",
    "            axes[1, 1].plot(history['epochs'], history['mean_diff'], 'c-', label='Mean Diff')\n",
    "            axes[1, 1].plot(history['epochs'], history['var_diff'], 'orange', label='Var Diff')\n",
    "            axes[1, 1].axvline(x=best_epoch, color='g', linestyle='--')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Difference')\n",
    "            axes[1, 1].set_title('Distribution Differences')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f'Final Training Summary - {experiment_name}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        final_plot_path = os.path.join(exp_plot_dir, \"final_summary.png\")\n",
    "        plt.savefig(final_plot_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"Final summary plot saved to: {final_plot_path}\")\n",
    "        checkpoint_dir = paths['checkpoint_dir']\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return checkpoint_path, best_val_loss, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training\n",
    "print(\"Running full training!!!\")\n",
    "results = run_single_experiment_with_plots(\n",
    "    CONFIG, {}, run_id=\"full_training\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Execution complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
